# 强化学习算法对比

* DQN原论文：Playing Atari with Deep Reinforcement Learning

  主要特点： Off-policy，Discrete action space，model free（这些概念参考博客链接一）

  （1）Experience replay（经验回放）：NN 的训练样本独立，而 RL 中的前后状态相关，所以需要做一点改动。因为 Q-Learning 是一个离线学习算法，所以在每次 DQN 更新时，可以随机选取一些过去的状态来进行学习。这种方式打乱了状态之间的相关性，可以使神经网络更有效率。

  （2）Fixed Q-target（固定 Q-目标）：因为网络会不断的更新，所以相同 s 和 a 下的 Q-target 和 Q-eval 的值是不固定的，这样训练起来比较困难。所以可以将 Q-target 固定住，这样目标问题就变成了一个回归问题：用 Q-eval 去逼近 Q-target。

  

* DDPG原论文： Continuous Control with Deep Reinforcement Learning

  主要特点：Model free， off policy， continuous action

  出现动机：DQN 中采用 Q-network 来进行 Value function approximation 解决了连续状态空间问题，但基于 critic-only method 自然是没法处理连续动作空间问题。 DDPG 是在处理连续动作问题上的拓展。显然是通过 actor-only 类的 policy-based 方法，利用策略梯度方法直接优化用深度神经网络参数化表示的 policy，即网络的输出就是动作。

  

* A2C/A3C原论文：Asynchronous Methods for Deep Reinforcement Learning

  主要特点：on policy, Discrete and continuous action space

  出现动机：前面的 DRL 训练时非常依赖于计算强劲的 GPU，而且沿袭了 DQN 提出的处理数据相关性的 experience replay 机制。但 experience replay 也存在缺陷：

  1. agent 与 env 的每次实时交互都需要耗费巨大的内存和算力
  2. experience replay 要求 agent 采用 off-policy，而 off-policy 只能基于旧策略生成的数据进行更新

  ​     面对这些问题，ARL 能利用 cpu 的多线程，实现多 agent 实例的并行 actor-learner 对，每个线程对应不同的探索策略，这种并行化起到了数据解相关的作用，可替代 experience replay 从而节省存储开销。

  ​     异步的实现，简单地说，就是等所有的异步 agent 执行完后，再用累计的梯度信息更新网络参数。其中，n-step 类算法需要每个异步 agent 复制一份子网络，每个 agent 执行 n 步后倒退算出每步的总回报和相关梯度，用累计梯度更新主网络参数。如果不复制子网络，则等单个 agent 执行完 n-step 耗时太多，而 one-step 可忽略这个影响）。

  

* PPO原论文：Proximal Policy Optimization Algorithms

  主要特点：on policy, actor critic, Both discrete continuous action space

  ​     本质上也是AC神经网络，Policy Gradient是基于Policy做梯度下降优化模型，进而改变动作出现的概率。PG算法的目的是使得策略πθ的期望最大化。



参考博客 ：

强化学习分类：https://blog.csdn.net/qq_33302004/article/details/112493945?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522166744514716800192240731%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=166744514716800192240731&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-1-112493945-null-null.142^v62^pc_rank_34_queryrelevant25,201^v3^control_1,213^v1^control&utm_term=强化学习分类&spm=1018.2226.3001.4187

https://blog.csdn.net/te_sun/article/details/105082305?spm=1001.2101.3001.6650.2&utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-2-105082305-blog-88038526.pc_relevant_landingrelevant&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-2-105082305-blog-88038526.pc_relevant_landingrelevant&utm_relevant_index=5